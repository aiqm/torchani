.. note::
    :class: sphx-glr-download-link-note

    Click :ref:`here <sphx_glr_download_examples_nnp_training_ignite.py>` to download the full example code
.. rst-class:: sphx-glr-example-title

.. _sphx_glr_examples_nnp_training_ignite.py:


.. _training-example-ignite:

Train Your Own Neural Network Potential, Using PyTorch-Ignite
=============================================================

We have seen how to train a neural network potential by manually writing
training loop in :ref:`training-example`. TorchANI provide tools to work
with PyTorch-Ignite to simplify the writing of training code. This tutorial
shows how to use these tools to train a demo model. The setup in this demo is
not necessarily identical to NeuroChem.

This tutorial assumes readers have read :ref:`training-example`.

To begin with, let's first import the modules and setup devices we will use:


.. code-block:: default

    import torch
    import ignite
    import torchani
    import timeit
    import os
    import ignite.contrib.handlers
    import torch.utils.tensorboard

    # device to run the training
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')








Now let's setup training hyperparameters and dataset.


.. code-block:: default


    # training and validation set
    try:
        path = os.path.dirname(os.path.realpath(__file__))
    except NameError:
        path = os.getcwd()
    dspath = os.path.join(path, '../dataset/ani1-up_to_gdb4/ani_gdb_s01.h5')

    # checkpoint file to save model when validation RMSE improves
    model_checkpoint = 'model.pt'

    # max epochs to run the training
    max_epochs = 20

    # Compute training RMSE every this steps. Since the training set is usually
    # huge and the loss funcition does not directly gives us RMSE, we need to
    # check the training RMSE to see overfitting.
    training_rmse_every = 5

    # batch size
    batch_size = 2560

    # log directory for tensorboard
    log = 'runs'








Instead of manually specifying hyperparameters as in :ref:`training-example`,
here we will load them from files.


.. code-block:: default

    const_file = os.path.join(path, '../torchani/resources/ani-1x_8x/rHCNO-5.2R_16-3.5A_a4-8.params')  # noqa: E501
    consts = torchani.neurochem.Constants(const_file)
    aev_computer = torchani.AEVComputer(**consts)
    energy_shifter = torchani.utils.EnergyShifter(None)








Now let's define atomic neural networks. Here in this demo, we use the same
size of neural network for all atom types, but this is not necessary.


.. code-block:: default

    def atomic():
        model = torch.nn.Sequential(
            torch.nn.Linear(384, 128),
            torch.nn.CELU(0.1),
            torch.nn.Linear(128, 128),
            torch.nn.CELU(0.1),
            torch.nn.Linear(128, 64),
            torch.nn.CELU(0.1),
            torch.nn.Linear(64, 1)
        )
        return model


    nn = torchani.ANIModel([atomic() for _ in range(4)])
    print(nn)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    ANIModel(
      (0): Sequential(
        (0): Linear(in_features=384, out_features=128, bias=True)
        (1): CELU(alpha=0.1)
        (2): Linear(in_features=128, out_features=128, bias=True)
        (3): CELU(alpha=0.1)
        (4): Linear(in_features=128, out_features=64, bias=True)
        (5): CELU(alpha=0.1)
        (6): Linear(in_features=64, out_features=1, bias=True)
      )
      (1): Sequential(
        (0): Linear(in_features=384, out_features=128, bias=True)
        (1): CELU(alpha=0.1)
        (2): Linear(in_features=128, out_features=128, bias=True)
        (3): CELU(alpha=0.1)
        (4): Linear(in_features=128, out_features=64, bias=True)
        (5): CELU(alpha=0.1)
        (6): Linear(in_features=64, out_features=1, bias=True)
      )
      (2): Sequential(
        (0): Linear(in_features=384, out_features=128, bias=True)
        (1): CELU(alpha=0.1)
        (2): Linear(in_features=128, out_features=128, bias=True)
        (3): CELU(alpha=0.1)
        (4): Linear(in_features=128, out_features=64, bias=True)
        (5): CELU(alpha=0.1)
        (6): Linear(in_features=64, out_features=1, bias=True)
      )
      (3): Sequential(
        (0): Linear(in_features=384, out_features=128, bias=True)
        (1): CELU(alpha=0.1)
        (2): Linear(in_features=128, out_features=128, bias=True)
        (3): CELU(alpha=0.1)
        (4): Linear(in_features=128, out_features=64, bias=True)
        (5): CELU(alpha=0.1)
        (6): Linear(in_features=64, out_features=1, bias=True)
      )
    )



If checkpoint from previous training exists, then load it.


.. code-block:: default

    if os.path.isfile(model_checkpoint):
        nn.load_state_dict(torch.load(model_checkpoint))
    else:
        torch.save(nn.state_dict(), model_checkpoint)







Let's now create a pipeline of AEV Computer --> Neural Networks.


.. code-block:: default

    model = torch.nn.Sequential(aev_computer, nn).to(device)







Now setup tensorboard


.. code-block:: default

    writer = torch.utils.tensorboard.SummaryWriter(log_dir=log)







Now load training and validation datasets into memory.


.. code-block:: default

    training, validation = torchani.data.load_ani_dataset(
        dspath, consts.species_to_tensor, batch_size, device=device,
        transform=[energy_shifter.subtract_from_dataset], split=[0.8, None])







We have tools to deal with the chunking (see :ref:`training-example`). These
tools can be used as follows:


.. code-block:: default

    container = torchani.ignite.Container({'energies': model})
    optimizer = torch.optim.Adam(model.parameters())
    trainer = ignite.engine.create_supervised_trainer(
        container, optimizer, torchani.ignite.MSELoss('energies'))
    evaluator = ignite.engine.create_supervised_evaluator(
        container,
        metrics={
            'RMSE': torchani.ignite.RMSEMetric('energies')
        })








Let's add a progress bar for the trainer


.. code-block:: default

    pbar = ignite.contrib.handlers.ProgressBar()
    pbar.attach(trainer)








And some event handlers to compute validation and training metrics:


.. code-block:: default

    def hartree2kcal(x):
        return 627.509 * x


    @trainer.on(ignite.engine.Events.EPOCH_STARTED)
    def validation_and_checkpoint(trainer):
        def evaluate(dataset, name):
            evaluator = ignite.engine.create_supervised_evaluator(
                container,
                metrics={
                    'RMSE': torchani.ignite.RMSEMetric('energies')
                }
            )
            evaluator.run(dataset)
            metrics = evaluator.state.metrics
            rmse = hartree2kcal(metrics['RMSE'])
            writer.add_scalar(name, rmse, trainer.state.epoch)

        # compute validation RMSE
        evaluate(validation, 'validation_rmse_vs_epoch')

        # compute training RMSE
        if trainer.state.epoch % training_rmse_every == 1:
            evaluate(training, 'training_rmse_vs_epoch')

        # checkpoint model
        torch.save(nn.state_dict(), model_checkpoint)








Also some to log elapsed time:


.. code-block:: default

    start = timeit.default_timer()


    @trainer.on(ignite.engine.Events.EPOCH_STARTED)
    def log_time(trainer):
        elapsed = round(timeit.default_timer() - start, 2)
        writer.add_scalar('time_vs_epoch', elapsed, trainer.state.epoch)








Also log the loss per iteration:


.. code-block:: default

    @trainer.on(ignite.engine.Events.ITERATION_COMPLETED)
    def log_loss(trainer):
        iteration = trainer.state.iteration
        writer.add_scalar('loss_vs_iteration', trainer.state.output, iteration)








And finally, we are ready to run:


.. code-block:: default

    trainer.run(training, max_epochs)




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [0/4]   0%|           [00:00<?]    Epoch [1/20]: [0/4]   0%|           [00:00<?]    Epoch [1/20]: [1/4]  25%|##5        [00:00<00:01]    Epoch [1/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [1/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [1/20]: [3/4]  75%|#######5   [00:00<00:00]    Epoch [1/20]: [3/4]  75%|#######5   [00:01<00:00]    Epoch [1/20]: [4/4] 100%|########## [00:01<00:00]                                                     [0/4]   0%|           [00:00<?]    Epoch [2/20]: [0/4]   0%|           [00:00<?]    Epoch [2/20]: [1/4]  25%|##5        [00:00<00:01]    Epoch [2/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [2/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [2/20]: [3/4]  75%|#######5   [00:00<00:00]    Epoch [2/20]: [3/4]  75%|#######5   [00:01<00:00]    Epoch [2/20]: [4/4] 100%|########## [00:01<00:00]                                                     [0/4]   0%|           [00:00<?]    Epoch [3/20]: [0/4]   0%|           [00:00<?]    Epoch [3/20]: [1/4]  25%|##5        [00:00<00:01]    Epoch [3/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [3/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [3/20]: [3/4]  75%|#######5   [00:00<00:00]    Epoch [3/20]: [3/4]  75%|#######5   [00:01<00:00]    Epoch [3/20]: [4/4] 100%|########## [00:01<00:00]                                                     [0/4]   0%|           [00:00<?]    Epoch [4/20]: [0/4]   0%|           [00:00<?]    Epoch [4/20]: [1/4]  25%|##5        [00:00<00:01]    Epoch [4/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [4/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [4/20]: [3/4]  75%|#######5   [00:00<00:00]    Epoch [4/20]: [3/4]  75%|#######5   [00:01<00:00]    Epoch [4/20]: [4/4] 100%|########## [00:01<00:00]                                                     [0/4]   0%|           [00:00<?]    Epoch [5/20]: [0/4]   0%|           [00:00<?]    Epoch [5/20]: [1/4]  25%|##5        [00:00<00:01]    Epoch [5/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [5/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [5/20]: [3/4]  75%|#######5   [00:00<00:00]    Epoch [5/20]: [3/4]  75%|#######5   [00:01<00:00]    Epoch [5/20]: [4/4] 100%|########## [00:01<00:00]                                                     [0/4]   0%|           [00:00<?]    Epoch [6/20]: [0/4]   0%|           [00:00<?]    Epoch [6/20]: [1/4]  25%|##5        [00:00<00:01]    Epoch [6/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [6/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [6/20]: [3/4]  75%|#######5   [00:00<00:00]    Epoch [6/20]: [3/4]  75%|#######5   [00:01<00:00]    Epoch [6/20]: [4/4] 100%|########## [00:01<00:00]                                                     [0/4]   0%|           [00:00<?]    Epoch [7/20]: [0/4]   0%|           [00:00<?]    Epoch [7/20]: [1/4]  25%|##5        [00:00<00:01]    Epoch [7/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [7/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [7/20]: [3/4]  75%|#######5   [00:00<00:00]    Epoch [7/20]: [3/4]  75%|#######5   [00:01<00:00]    Epoch [7/20]: [4/4] 100%|########## [00:01<00:00]                                                     [0/4]   0%|           [00:00<?]    Epoch [8/20]: [0/4]   0%|           [00:00<?]    Epoch [8/20]: [1/4]  25%|##5        [00:00<00:01]    Epoch [8/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [8/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [8/20]: [3/4]  75%|#######5   [00:00<00:00]    Epoch [8/20]: [3/4]  75%|#######5   [00:01<00:00]    Epoch [8/20]: [4/4] 100%|########## [00:01<00:00]                                                     [0/4]   0%|           [00:00<?]    Epoch [9/20]: [0/4]   0%|           [00:00<?]    Epoch [9/20]: [1/4]  25%|##5        [00:00<00:01]    Epoch [9/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [9/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [9/20]: [3/4]  75%|#######5   [00:00<00:00]    Epoch [9/20]: [3/4]  75%|#######5   [00:01<00:00]    Epoch [9/20]: [4/4] 100%|########## [00:01<00:00]                                                     [0/4]   0%|           [00:00<?]    Epoch [10/20]: [0/4]   0%|           [00:00<?]    Epoch [10/20]: [1/4]  25%|##5        [00:00<00:01]    Epoch [10/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [10/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [10/20]: [3/4]  75%|#######5   [00:00<00:00]    Epoch [10/20]: [3/4]  75%|#######5   [00:01<00:00]    Epoch [10/20]: [4/4] 100%|########## [00:01<00:00]                                                      [0/4]   0%|           [00:00<?]    Epoch [11/20]: [0/4]   0%|           [00:00<?]    Epoch [11/20]: [1/4]  25%|##5        [00:00<00:01]    Epoch [11/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [11/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [11/20]: [3/4]  75%|#######5   [00:00<00:00]    Epoch [11/20]: [3/4]  75%|#######5   [00:01<00:00]    Epoch [11/20]: [4/4] 100%|########## [00:01<00:00]                                                      [0/4]   0%|           [00:00<?]    Epoch [12/20]: [0/4]   0%|           [00:00<?]    Epoch [12/20]: [1/4]  25%|##5        [00:00<00:01]    Epoch [12/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [12/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [12/20]: [3/4]  75%|#######5   [00:00<00:00]    Epoch [12/20]: [3/4]  75%|#######5   [00:01<00:00]    Epoch [12/20]: [4/4] 100%|########## [00:01<00:00]                                                      [0/4]   0%|           [00:00<?]    Epoch [13/20]: [0/4]   0%|           [00:00<?]    Epoch [13/20]: [1/4]  25%|##5        [00:00<00:01]    Epoch [13/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [13/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [13/20]: [3/4]  75%|#######5   [00:00<00:00]    Epoch [13/20]: [3/4]  75%|#######5   [00:01<00:00]    Epoch [13/20]: [4/4] 100%|########## [00:01<00:00]                                                      [0/4]   0%|           [00:00<?]    Epoch [14/20]: [0/4]   0%|           [00:00<?]    Epoch [14/20]: [1/4]  25%|##5        [00:00<00:01]    Epoch [14/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [14/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [14/20]: [3/4]  75%|#######5   [00:00<00:00]    Epoch [14/20]: [3/4]  75%|#######5   [00:01<00:00]    Epoch [14/20]: [4/4] 100%|########## [00:01<00:00]                                                      [0/4]   0%|           [00:00<?]    Epoch [15/20]: [0/4]   0%|           [00:00<?]    Epoch [15/20]: [1/4]  25%|##5        [00:00<00:01]    Epoch [15/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [15/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [15/20]: [3/4]  75%|#######5   [00:00<00:00]    Epoch [15/20]: [3/4]  75%|#######5   [00:01<00:00]    Epoch [15/20]: [4/4] 100%|########## [00:01<00:00]                                                      [0/4]   0%|           [00:00<?]    Epoch [16/20]: [0/4]   0%|           [00:00<?]    Epoch [16/20]: [1/4]  25%|##5        [00:00<00:01]    Epoch [16/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [16/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [16/20]: [3/4]  75%|#######5   [00:00<00:00]    Epoch [16/20]: [3/4]  75%|#######5   [00:01<00:00]    Epoch [16/20]: [4/4] 100%|########## [00:01<00:00]                                                      [0/4]   0%|           [00:00<?]    Epoch [17/20]: [0/4]   0%|           [00:00<?]    Epoch [17/20]: [1/4]  25%|##5        [00:00<00:01]    Epoch [17/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [17/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [17/20]: [3/4]  75%|#######5   [00:00<00:00]    Epoch [17/20]: [3/4]  75%|#######5   [00:01<00:00]    Epoch [17/20]: [4/4] 100%|########## [00:01<00:00]                                                      [0/4]   0%|           [00:00<?]    Epoch [18/20]: [0/4]   0%|           [00:00<?]    Epoch [18/20]: [1/4]  25%|##5        [00:00<00:01]    Epoch [18/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [18/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [18/20]: [3/4]  75%|#######5   [00:00<00:00]    Epoch [18/20]: [3/4]  75%|#######5   [00:01<00:00]    Epoch [18/20]: [4/4] 100%|########## [00:01<00:00]                                                      [0/4]   0%|           [00:00<?]    Epoch [19/20]: [0/4]   0%|           [00:00<?]    Epoch [19/20]: [1/4]  25%|##5        [00:00<00:01]    Epoch [19/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [19/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [19/20]: [3/4]  75%|#######5   [00:00<00:00]    Epoch [19/20]: [3/4]  75%|#######5   [00:01<00:00]    Epoch [19/20]: [4/4] 100%|########## [00:01<00:00]                                                      [0/4]   0%|           [00:00<?]    Epoch [20/20]: [0/4]   0%|           [00:00<?]    Epoch [20/20]: [1/4]  25%|##5        [00:00<00:01]    Epoch [20/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [20/20]: [2/4]  50%|#####      [00:00<00:00]    Epoch [20/20]: [3/4]  75%|#######5   [00:00<00:00]    Epoch [20/20]: [3/4]  75%|#######5   [00:01<00:00]    Epoch [20/20]: [4/4] 100%|########## [00:01<00:00]                                                  



.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  37.703 seconds)


.. _sphx_glr_download_examples_nnp_training_ignite.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download

     :download:`Download Python source code: nnp_training_ignite.py <nnp_training_ignite.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: nnp_training_ignite.ipynb <nnp_training_ignite.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
