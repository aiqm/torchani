{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Basic usage of :obj:`~torchani.datasets.ANIDataset`\n\nThis supersedes the obsolete anidataloader. There are also builtin datasets\nthat live in moria, and they can be directly downloaded through torchani.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# To begin with, let's import the modules we will use:\nimport shutil\nfrom pathlib import Path\n\nfrom torchani.datasets import ANIDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Downloading the builtin datasets performs a checksum to make sure the files\nare correct. If the function is called again and the dataset is already on\nthe path, only the checksum is performed, the data is not downloaded. The\noutput is an ANIDataset class\nUncomment the following code to download (watch out, it may take some time):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# import torchani\n# ds_1x = torchani.datasets.ANI1x('./datasets/ani1x/', download=True)\n# ds_comp6 = torchani.datasets.COMP6v1('./datasets/comp6v1/', download=True)\n# ds_2x = torchani.datasets.ANI2x('./datasets/ani2x/', download=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the purposes of this example we will copy and modify two files inside\ntorchani/dataset, which can be downloaded by running the download.sh script\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "file1_path = Path.cwd() / \"file1.h5\"\nfile2_path = Path.cwd() / \"file2.h5\"\nshutil.copy(Path.cwd() / \"../dataset/ani1-up_to_gdb4/ani_gdb_s01.h5\", file1_path)\nshutil.copy(Path.cwd() / \"../dataset/ani1-up_to_gdb4/ani_gdb_s02.h5\", file2_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ANIDataset accepts a path to an h5 file or a list of paths to many files\n(optionally with names)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ds = ANIDataset(locations=(file1_path, file2_path), names=(\"file1\", \"file2\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ANIDatasets have properties they can access. All conformers in the dataset\nhave the same set of properties, lets check what properties this dataset\nholds\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(ds.properties)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When opening these files we see that we get a warning because they have some\nunsupported legacy properties, so the first thing we will do is delete them\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ds.delete_properties((\"coordinatesHE\", \"energiesHE\", \"smiles\"))\nprint(ds.properties)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conformer groups\n\nTo access groups of conformers we can just use the dataset as an ordered\ndictionary\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "group = ds[\"file2/gdb11_s02/gdb11_s02-8\"]\nprint(group)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that we get some tensors with properties, but this access is not very\nconvenient, the keys seem to have weird mangled names which don't say very\nmuch about what is in them.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(list(ds.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is because this dataset is in a legacy format, we can check that\nby querying the \"grouping\"\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(ds.grouping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before moving on, lets reformat this dataset so that it is in a more\nstandarized format\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ds.regroup_by_formula()\nprint(list(ds.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now the dataset is organized by formulas, which makes access much easier\n(If we only had one file ds['CH4'] would have been enough)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "group = ds[\"file1/CH4\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "items(), values() and keys() work as expected for groups of conformers,\nhere we print only the first 100 as a sample\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for j, (k, v) in enumerate(ds.items()):\n    print(k, v)\n    if j == 10:\n        break\n\nfor j, k in enumerate(ds.keys()):\n    print(k)\n    if j == 10:\n        break\n\nfor j, v in enumerate(ds.values()):\n    print(v)\n    if j == 10:\n        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To get the number of groups of conformers we can use len(), or also\ndataset.num_conformer_groups\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_groups = len(ds)\nprint(num_groups)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To get the number of conformers we can use num_conformers\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_conformers = ds.num_conformers\nprint(num_conformers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conformers\n\nTo access individual conformers or subsets of conformers we use \"conformer\"\nmethods, get_conformers and iter_conformers\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "conformer = ds.get_conformers(\"file1/CH4\", 0)\nprint(conformer)\nconformer = ds.get_conformers(\"file1/CH4\", 1)\nprint(conformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A tensor / list / array can also be passed for indexing, to fetch multiple\nconformers from the same group, which is faster. Since we copy the data forh\nsimplicity, this allows all fancy indexing operations (directly indexing\nusing h5py for example does not).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "conformers = ds.get_conformers(\"file1/CH4\", [0, 1])\nprint(conformers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also access all the group if we don't pass an index, same as normal indexing\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "conformer = ds.get_conformers(\"file1/CH4\")\nprint(conformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, it is possible to also specify which properties we want using 'properties'\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "conformer = ds.get_conformers(\"file1/CH4\", [0, 3], properties=(\"species\", \"energies\"))\nprint(conformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you want you can also get the conformers as numpy arrays by calling\nget_numpy_conformers.  this has an optional flag \"chem_symbols\" which if\nspecified \"True\" will output the elements as strings ('C', 'H', 'H', ... etc)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "conformer = ds.get_numpy_conformers(\"file1/CH4\", [0, 1], chem_symbols=True)\nprint(conformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can iterate over all conformers sequentially by calling iter_conformer,\n(this is faster than doing it manually since it caches each conformer group\nprevious to starting the iteration), here we print the first 100 as a sample\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for c in ds.iter_conformers(limit=100):\n    print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now delete the files we copied for cleanup purposes\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "file1_path.unlink()\nfile2_path.unlink()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}