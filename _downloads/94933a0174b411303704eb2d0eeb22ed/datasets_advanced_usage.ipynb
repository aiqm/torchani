{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Advanced usage of :obj:`~torchani.datasets.ANIDataset`\n\nExample showing more involved conformer and property manipulation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To begin with, let's import the modules we will use:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import shutil\nfrom pathlib import Path\n\nimport torch\nimport numpy as np\n\nfrom torchani.datasets import ANIDataset, concatenate\nfrom torchani.datasets.filters import filter_by_high_force"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again for the purposes of this example we will copy and modify two files\ninside torchani/dataset, which can be downloaded by running the download.sh\nscript.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "file1_path = Path.cwd() / \"file1.h5\"\nfile2_path = Path.cwd() / \"file2.h5\"\nshutil.copy(Path.cwd() / \"../dataset/ani1-up_to_gdb4/ani_gdb_s01.h5\", file1_path)\nshutil.copy(Path.cwd() / \"../dataset/ani1-up_to_gdb4/ani_gdb_s02.h5\", file2_path)\nds = ANIDataset(locations=(file1_path, file2_path), names=(\"file1\", \"file2\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Property deletion / renaming\n\nAll of the molecules in the dataset have the same properties, energies,\ncoordinates, etc. You can query which are these.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ds.properties"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is possible to delete unwanted / unnedded properties.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ds.delete_properties((\"coordinatesHE\", \"energiesHE\", \"smiles\"))\nds.properties"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is also possible to rename the properties by passing a dict of old-new names (the\nclass assumes at least one of \"species\" or \"numbers\" is always present, so don't\nrename those).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ds.rename_properties({\"energies\": \"molecular_energies\", \"coordinates\": \"coord\"})\nds.properties"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets rename them back to their original values:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ds.rename_properties({\"molecular_energies\": \"energies\", \"coord\": \"coordinates\"})\nds.properties"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Grouping\n\nYou can query whether your dataset is in a legacy format by interrogating the\ndataset grouping attribute\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ds.grouping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Legacy format is the format used by some old datasets. In the legacy format\nthere can be groups arbitrarily nested in the hierarchical tree inside the h5\nfiles, and the \"species\"/\"numbers\" property does not have a batch dimension.\nThis means all properties with an \"atomic\" dimension must be ordered the same\nway within a group (don't worry too much if you don't understand what this\nmeans, it basically means this is difficult to deal with)\n\nWe can convert to a less error prone and easier to parse format by calling\n\"regroup_by_formula\" or \"regroup_by_num_atoms\"\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ds = ds.regroup_by_formula()\nds.grouping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another possibility is to group by num atoms\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ds = ds.regroup_by_num_atoms()\nds.grouping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In these formats all of the first dimensions of all properties are the same\nin all groups, and groups can only have depth one. In other words the tree\nstructure is, for \"by_formula\" ::\n\n   /C10H22/coordinates, shape (10, 32, 3)\n          /species, shape (10, 32)\n          /energies, shape (10,)\n   /C8H22N2/coordinates, shape (10, 32, 3)\n          /species, shape (10, 32)\n          /energies, shape (10,)\n   /C12H22/coordinates, shape (5, 34, 3)\n          /species, shape (5, 34)\n          /energies, shape (5,)\n\nand for, \"by_num_atoms\" ::\n\n   /032/coordinates, shape (20, 32, 3)\n        /species, shape (20, 32)\n        /energies, shape (20,)\n   /034/coordinates, shape (5, 34, 3)\n        /species, shape (5, 34)\n        /energies, shape (5,)\n\nConformer groups can be iterated over in chunks, up to a specified maximum\nchunk size. This breaks a conformer group into mini-batches containing\nmultiple inputs, allowing the dataset to be iterated over much more\nefficiently. As we regrouped the dataset by num_atoms in the previous step,\nthis will iterate over conformer groups containing the same number of atoms.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with ds.keep_open(\"r\") as read_ds:\n    for group, j, conformer in read_ds.chunked_items(max_size=1500, limit=2):\n        species = conformer[\"species\"]\n        coordinates = conformer[\"coordinates\"]\n        ani_input = (species, coordinates)\n        print(ani_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Property creation\n\nSometimes it may be useful to just create one placeholder property for some\npurpose. You can make the second dimension equal to the number of atoms in\nthe group by setting ``is_atomic=True``, and you can add also extra dims, for\nexample, this creates a property with shape ``(N, A)``, for more examples see\ndocstring of the function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ds = ds.create_full_property(\n    \"new_property\", is_atomic=True, fill_value=0.0, dtype=float\n)\nds.properties"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now delete the created property for cleanup\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ds.delete_properties(\"new_property\", verbose=False)\nds.properties"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Manipulating conformers\n\nAll of the molecules in the dataset have the same properties\nConformers as tensors can be appended by calling ``append_conformers``.\nHere I put random numbers as species and coordinates but you should put\nsomething that makes sense, if you have only one store you can pass\n\"group_name\" directly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "conformers = {\n    \"species\": torch.tensor([[1, 1, 6, 6], [1, 1, 6, 6]]),\n    \"coordinates\": torch.randn(2, 4, 3),\n    \"energies\": torch.randn(2),\n}\nds.append_conformers(\"file1/004\", conformers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is also possible to append conformers as numpy arrays, in this case\n\"species\" can hold the chemical symbols or atomic numbers. Internally these\nwill be converted to atomic numbers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "numpy_conformers = {\n    \"species\": np.array(\n        [[\"H\", \"H\", \"C\", \"N\"], [\"H\", \"H\", \"N\", \"O\"], [\"H\", \"H\", \"H\", \"H\"]]\n    ),\n    \"coordinates\": np.random.standard_normal((3, 4, 3)),\n    \"energies\": np.random.standard_normal(3),\n}\nds.append_conformers(\"file1/004\", numpy_conformers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conformers can also be deleted from the dataset. Passing an index will delete\na series of conformers, not passing anything deletes the whole group\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "molecules = ds.get_conformers(\"file1/004\")\nmolecules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets delete some conformers and try again\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ds.delete_conformers(\"file1/004\", [0, 2])\nmolecules = ds.get_conformers(\"file1/004\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The len of the dataset has not changed\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "len(ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets get rid of the whole group\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ds.delete_conformers(\"file1/004\")\nlen(ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Currently, when appending the class checks:\n\n- That the first dimension of all your properties is the same\n- That you are appending a set of conformers with correct properties\n- That all your formulas are correct when the grouping type is \"by_formula\",\n- That your group name does not contain illegal \"/\" characters\n- That you are only appending one of \"species\" or \"numbers\"\n\nIt does NOT check:\n\n- That the number of atoms is the same in all properties that are atomic\n- That the name of the group is consistent with the formula / num atoms\n\nIt is the responsibility of the user to make sure of those items.\n\n## Utilities\n\nMultiple datasets can be concatenated into one h5 file, optionally deleting the\noriginal h5 files if the concatenation is successful.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "concat_path = Path.cwd() / \"concat.h5\"\nds = concatenate(ds, concat_path, delete_originals=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Context manager usage\n\nIf you need to perform a lot of read/write operations in the dataset it can\nbe useful to keep all the underlying stores open, you can do this by using a\n``keep_open`` context.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with ds.keep_open(\"r+\") as open_ds:\n    for c in open_ds.iter_conformers(limit=10):\n        print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a dataset from scratch\n\nIt is possible to create an ANIDataset from scratch by calling: By defalt the\ngrouping is \"by_num_atoms\". The first set of conformers you append will\ndetermine what properties this dataset will support.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "new_path = Path.cwd() / \"new_ds.h5\"\nnew_ds = ANIDataset(new_path, grouping=\"by_formula\")\nnumpy_conformers = {\n    \"species\": np.array([[\"H\", \"H\", \"C\", \"C\"], [\"H\", \"C\", \"H\", \"C\"]]),\n    \"coordinates\": np.random.standard_normal((2, 4, 3)),\n    \"forces\": np.random.normal(size=(2, 4, 3), scale=0.1),\n    \"dipoles\": np.random.standard_normal((2, 3)),\n    \"energies\": np.random.standard_normal(2),\n}\nnew_ds.append_conformers(\"C2H2\", numpy_conformers)\nprint(new_ds.properties)\nfor c in new_ds.iter_conformers():\n    print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another useful feature is deleting inplace all conformers with force\nmagnitude above a given threshold, we will exemplify this by introducing some\nconformers with extremely large forces\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bad_conformers = {\n    \"species\": np.array([[\"H\", \"H\", \"N\", \"N\"], [\"H\", \"H\", \"N\", \"N\"]]),\n    \"coordinates\": np.random.standard_normal((2, 4, 3)),\n    \"forces\": np.random.normal(size=(2, 4, 3), scale=100.0),\n    \"dipoles\": np.random.standard_normal((2, 3)),\n    \"energies\": np.random.standard_normal(2),\n}\nnew_ds.append_conformers(\"C2H2\", bad_conformers)\nfiltered_conformers_and_ids = filter_by_high_force(new_ds, delete_inplace=True)\nfiltered_conformers_and_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, lets delete the files we used for cleanup\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "concat_path.unlink()\nnew_path.unlink()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}