{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\nTrain Your Own Neural Network Potential\n=======================================\n\nThis example shows how to use TorchANI to train a neural network potential. We\nwill use the same configuration as specified as in `inputtrain.ipt`_\n\n    https://github.com/aiqm/torchani/blob/master/torchani/resources/ani-1x_8x/inputtrain.ipt\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>TorchANI provide tools to run NeuroChem training config file `inputtrain.ipt`.\n    See: `neurochem-training`.</p></div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To begin with, let's first import the modules and setup devices we will use:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torchani\nimport os\nimport math\nimport torch.utils.tensorboard\nimport tqdm\n\n# device to run the training\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's setup constants and construct an AEV computer. These numbers could\nbe found in `rHCNO-5.2R_16-3.5A_a4-8.params`_ and `sae_linfit.dat`_\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Besides defining these hyperparameters programmatically,\n  :mod:`torchani.neurochem` provide tools to read them from file. See also\n  `training-example-ignite` for an example of usage.</p></div>\n\n  https://github.com/aiqm/torchani/blob/master/torchani/resources/ani-1x_8x/rHCNO-5.2R_16-3.5A_a4-8.params\n  https://github.com/aiqm/torchani/blob/master/torchani/resources/ani-1x_8x/sae_linfit.dat\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Rcr = 5.2000e+00\nRca = 3.5000e+00\nEtaR = torch.tensor([1.6000000e+01], device=device)\nShfR = torch.tensor([9.0000000e-01, 1.1687500e+00, 1.4375000e+00, 1.7062500e+00, 1.9750000e+00, 2.2437500e+00, 2.5125000e+00, 2.7812500e+00, 3.0500000e+00, 3.3187500e+00, 3.5875000e+00, 3.8562500e+00, 4.1250000e+00, 4.3937500e+00, 4.6625000e+00, 4.9312500e+00], device=device)\nZeta = torch.tensor([3.2000000e+01], device=device)\nShfZ = torch.tensor([1.9634954e-01, 5.8904862e-01, 9.8174770e-01, 1.3744468e+00, 1.7671459e+00, 2.1598449e+00, 2.5525440e+00, 2.9452431e+00], device=device)\nEtaA = torch.tensor([8.0000000e+00], device=device)\nShfA = torch.tensor([9.0000000e-01, 1.5500000e+00, 2.2000000e+00, 2.8500000e+00], device=device)\nnum_species = 4\naev_computer = torchani.AEVComputer(Rcr, Rca, EtaR, ShfR, EtaA, Zeta, ShfA, ShfZ, num_species)\nenergy_shifter = torchani.utils.EnergyShifter([\n    -0.600952980000,  # H\n    -38.08316124000,  # C\n    -54.70775770000,  # N\n    -75.19446356000,  # O\n])\nspecies_to_tensor = torchani.utils.ChemicalSymbolsToInts('HCNO')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's setup datasets. Note that here for our demo purpose, we set both\ntraining set and validation set the ``ani_gdb_s01.h5`` in TorchANI's\nrepository. This allows this program to finish very quick, because that\ndataset is very small. But this is wrong and should be avoided for any\nserious training. These paths assumes the user run this script under the\n``examples`` directory of TorchANI's repository. If you download this script,\nyou should manually set the path of these files in your system before this\nscript can run successfully.\n\nAlso note that we need to subtracting energies by the self energies of all\natoms for each molecule. This makes the range of energies in a reasonable\nrange. The second argument defines how to convert species as a list of string\nto tensor, that is, for all supported chemical symbols, which is correspond to\n``0``, which correspond to ``1``, etc.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    path = os.path.dirname(os.path.realpath(__file__))\nexcept NameError:\n    path = os.getcwd()\ntraining_path = os.path.join(path, '../dataset/ani1-up_to_gdb4/ani_gdb_s01.h5')\nvalidation_path = os.path.join(path, '../dataset/ani1-up_to_gdb4/ani_gdb_s01.h5')\n\nbatch_size = 2560\n\ntraining = torchani.data.BatchedANIDataset(\n    training_path, species_to_tensor, batch_size, device=device,\n    transform=[energy_shifter.subtract_from_dataset])\n\nvalidation = torchani.data.BatchedANIDataset(\n    validation_path, species_to_tensor, batch_size, device=device,\n    transform=[energy_shifter.subtract_from_dataset])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When iterating the dataset, we will get pairs of input and output\n``(species_coordinates, properties)``, where ``species_coordinates`` is the\ninput and ``properties`` is the output.\n\n``species_coordinates`` is a list of species-coordinate pairs, with shape\n``(N, Na)`` and ``(N, Na, 3)``. The reason for getting this type is, when\nloading the dataset and generating minibatches, the whole dataset are\nshuffled and each minibatch contains structures of molecules with a wide\nrange of number of atoms. Molecules of different number of atoms are batched\ninto single by padding. The way padding works is: adding ghost atoms, with\nspecies 'X', and do computations as if they were normal atoms. But when\ncomputing AEVs, atoms with species `X` would be ignored. To avoid computation\nwasting on padding atoms, minibatches are further splitted into chunks. Each\nchunk contains structures of molecules of similar size, which minimize the\ntotal number of padding atoms required to add. The input list\n``species_coordinates`` contains chunks of that minibatch we are getting. The\nbatching and chunking happens automatically, so the user does not need to\nworry how to construct chunks, but the user need to compute the energies for\neach chunk and concat them into single tensor.\n\nThe output, i.e. ``properties`` is a dictionary holding each property. This\nallows us to extend TorchANI in the future to training forces and properties.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's define atomic neural networks.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "H_network = torch.nn.Sequential(\n    torch.nn.Linear(384, 160),\n    torch.nn.CELU(0.1),\n    torch.nn.Linear(160, 128),\n    torch.nn.CELU(0.1),\n    torch.nn.Linear(128, 96),\n    torch.nn.CELU(0.1),\n    torch.nn.Linear(96, 1)\n)\n\nC_network = torch.nn.Sequential(\n    torch.nn.Linear(384, 144),\n    torch.nn.CELU(0.1),\n    torch.nn.Linear(144, 112),\n    torch.nn.CELU(0.1),\n    torch.nn.Linear(112, 96),\n    torch.nn.CELU(0.1),\n    torch.nn.Linear(96, 1)\n)\n\nN_network = torch.nn.Sequential(\n    torch.nn.Linear(384, 128),\n    torch.nn.CELU(0.1),\n    torch.nn.Linear(128, 112),\n    torch.nn.CELU(0.1),\n    torch.nn.Linear(112, 96),\n    torch.nn.CELU(0.1),\n    torch.nn.Linear(96, 1)\n)\n\nO_network = torch.nn.Sequential(\n    torch.nn.Linear(384, 128),\n    torch.nn.CELU(0.1),\n    torch.nn.Linear(128, 112),\n    torch.nn.CELU(0.1),\n    torch.nn.Linear(112, 96),\n    torch.nn.CELU(0.1),\n    torch.nn.Linear(96, 1)\n)\n\nnn = torchani.ANIModel([H_network, C_network, N_network, O_network])\nprint(nn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now create a pipeline of AEV Computer --> Neural Networks.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = torch.nn.Sequential(aev_computer, nn).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's setup the optimizer. We need to specify different weight decay rate\nfor different parameters. Since PyTorch does not have correct implementation\nof weight decay right now, we provide the correct implementation at TorchANI.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The weight decay in `inputtrain.ipt`_ is named \"l2\", but it is actually not\n  L2 regularization. The confusion between L2 and weight decay is a common\n  mistake in deep learning.  See: `Decoupled Weight Decay Regularization`_\n  Also note that the weight decay only applies to weight in the training\n  of ANI models, not bias.</p></div>\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>Currently TorchANI training with weight decay can not reproduce the training\n  result of NeuroChem with the same training setup. If you really want to use\n  weight decay, consider smaller rates and and make sure you do enough validation\n  to check if you get expected result.</p></div>\n\n  https://arxiv.org/abs/1711.05101\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = torchani.optim.AdamW([\n    # H networks\n    {'params': [H_network[0].weight], 'weight_decay': 0.0001},\n    {'params': [H_network[0].bias]},\n    {'params': [H_network[2].weight], 'weight_decay': 0.00001},\n    {'params': [H_network[2].bias]},\n    {'params': [H_network[4].weight], 'weight_decay': 0.000001},\n    {'params': [H_network[4].bias]},\n    {'params': H_network[6].parameters()},\n    # C networks\n    {'params': [C_network[0].weight], 'weight_decay': 0.0001},\n    {'params': [C_network[0].bias]},\n    {'params': [C_network[2].weight], 'weight_decay': 0.00001},\n    {'params': [C_network[2].bias]},\n    {'params': [C_network[4].weight], 'weight_decay': 0.000001},\n    {'params': [C_network[4].bias]},\n    {'params': C_network[6].parameters()},\n    # N networks\n    {'params': [N_network[0].weight], 'weight_decay': 0.0001},\n    {'params': [N_network[0].bias]},\n    {'params': [N_network[2].weight], 'weight_decay': 0.00001},\n    {'params': [N_network[2].bias]},\n    {'params': [N_network[4].weight], 'weight_decay': 0.000001},\n    {'params': [N_network[4].bias]},\n    {'params': N_network[6].parameters()},\n    # O networks\n    {'params': [O_network[0].weight], 'weight_decay': 0.0001},\n    {'params': [O_network[0].bias]},\n    {'params': [O_network[2].weight], 'weight_decay': 0.00001},\n    {'params': [O_network[2].bias]},\n    {'params': [O_network[4].weight], 'weight_decay': 0.000001},\n    {'params': [O_network[4].bias]},\n    {'params': O_network[6].parameters()},\n])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The way ANI trains a neural network potential looks like this:\n\nPhase 1: Pretrain the model by minimizing MSE loss\n\nPhase 2: Train the model by minimizing the exponential loss, until validation\nRMSE no longer improves for a certain steps, decay the learning rate and repeat\nthe same process, stop until the learning rate is smaller than a certain number.\n\nWe first read the checkpoint files to find where we are. We use `latest.pt`\nto store current training state. If `latest.pt` does not exist, this\nthis means the pretraining has not been finished yet.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "latest_checkpoint = 'latest.pt'\npretrained = os.path.isfile(latest_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "During training, we need to validate on validation set and if validation error\nis better than the best, then save the new best model to a checkpoint\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# helper function to convert energy unit from Hartree to kcal/mol\ndef hartree2kcal(x):\n    return 627.509 * x\n\n\ndef validate():\n    # run validation\n    mse_sum = torch.nn.MSELoss(reduction='sum')\n    total_mse = 0.0\n    count = 0\n    for batch_x, batch_y in validation:\n        true_energies = batch_y['energies']\n        predicted_energies = []\n        for chunk_species, chunk_coordinates in batch_x:\n            _, chunk_energies = model((chunk_species, chunk_coordinates))\n            predicted_energies.append(chunk_energies)\n        predicted_energies = torch.cat(predicted_energies)\n        total_mse += mse_sum(predicted_energies, true_energies).item()\n        count += predicted_energies.shape[0]\n    return hartree2kcal(math.sqrt(total_mse / count))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the model is not pretrained yet, we need to run the pretrain.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pretrain_criterion = 10  # kcal/mol\nmse = torch.nn.MSELoss(reduction='none')\n\nif not pretrained:\n    print(\"pre-training...\")\n    epoch = 0\n    rmse = math.inf\n    pretrain_optimizer = torch.optim.Adam(nn.parameters())\n    while rmse > pretrain_criterion:\n        for batch_x, batch_y in tqdm.tqdm(training):\n            true_energies = batch_y['energies']\n            predicted_energies = []\n            num_atoms = []\n            for chunk_species, chunk_coordinates in batch_x:\n                num_atoms.append((chunk_species >= 0).sum(dim=1))\n                _, chunk_energies = model((chunk_species, chunk_coordinates))\n                predicted_energies.append(chunk_energies)\n            num_atoms = torch.cat(num_atoms).to(true_energies.dtype)\n            predicted_energies = torch.cat(predicted_energies)\n            loss = (mse(predicted_energies, true_energies) / num_atoms).mean()\n            pretrain_optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        rmse = validate()\n        print('RMSE:', rmse, 'Target RMSE:', pretrain_criterion)\n    torch.save({\n        'nn': nn.state_dict(),\n        'optimizer': optimizer.state_dict(),\n    }, latest_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For phase 2, we need a learning rate scheduler to do learning rate decay\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will also use TensorBoard to visualize our training process\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensorboard = torch.utils.tensorboard.SummaryWriter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Resume training from previously saved checkpoints:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load(latest_checkpoint)\nnn.load_state_dict(checkpoint['nn'])\noptimizer.load_state_dict(checkpoint['optimizer'])\nif 'scheduler' in checkpoint:\n    scheduler.load_state_dict(checkpoint['scheduler'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we come to the training loop.\n\nIn this tutorial, we are setting the maximum epoch to a very small number,\nonly to make this demo terminate fast. For serious training, this should be\nset to a much larger value\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"training starting from epoch\", scheduler.last_epoch + 1)\nmax_epochs = 200\nearly_stopping_learning_rate = 1.0E-5\nbest_model_checkpoint = 'best.pt'\n\nfor _ in range(scheduler.last_epoch + 1, max_epochs):\n    rmse = validate()\n    learning_rate = optimizer.param_groups[0]['lr']\n\n    if learning_rate < early_stopping_learning_rate:\n        break\n\n    tensorboard.add_scalar('validation_rmse', rmse, scheduler.last_epoch)\n    tensorboard.add_scalar('best_validation_rmse', scheduler.best, scheduler.last_epoch)\n    tensorboard.add_scalar('learning_rate', learning_rate, scheduler.last_epoch)\n\n    # checkpoint\n    if scheduler.is_better(rmse, scheduler.best):\n        torch.save(nn.state_dict(), best_model_checkpoint)\n\n    scheduler.step(rmse)\n\n    for i, (batch_x, batch_y) in tqdm.tqdm(enumerate(training), total=len(training)):\n        true_energies = batch_y['energies']\n        predicted_energies = []\n        num_atoms = []\n        for chunk_species, chunk_coordinates in batch_x:\n            num_atoms.append((chunk_species >= 0).sum(dim=1))\n            _, chunk_energies = model((chunk_species, chunk_coordinates))\n            predicted_energies.append(chunk_energies)\n        num_atoms = torch.cat(num_atoms).to(true_energies.dtype)\n        predicted_energies = torch.cat(predicted_energies)\n        loss = (mse(predicted_energies, true_energies) / num_atoms).mean()\n        loss = 0.5 * (torch.exp(2 * loss) - 1)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # write current batch loss to TensorBoard\n        tensorboard.add_scalar('batch_loss', loss, scheduler.last_epoch * len(training) + i)\n\n    torch.save({\n        'nn': nn.state_dict(),\n        'optimizer': optimizer.state_dict(),\n        'scheduler': scheduler.state_dict(),\n    }, latest_checkpoint)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}