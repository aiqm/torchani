{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Using TorchScript to serialize models\n\nAll built-in models and modules in the `torchani` library, support TorchScript\nserialization, which is a native PyTorch feature where a python model is translated into\na PyTorch-specific format. If you use TorchScript you can load the resulting serialized\nfiles in a process where there is no Python dependency.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# To begin with, let's first import the modules we will use:\nfrom pathlib import Path\nimport typing as tp\n\nimport torch\nfrom torch import Tensor\n\nimport torchani\nfrom torchani.grad import hessians, forces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scripting an ANI model directly\n\nLet's now load the built-in ANI-1ccx models. The ANI-2x model contains 8\nmodels trained with diffrent initialization and on different splits of a dataset\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = torchani.models.ANI2x()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is very easy to compile and save the model using `torch.jit`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "compiled_model = torch.jit.script(model)\ntorch.jit.save(compiled_model, \"compiled_model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For testing purposes, we will now load the model we just saved and see if\nthey produces the same output as the original model:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loaded_compiled_model = torch.jit.load(\"compiled_model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use the molecule below to test:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "coordinates = torch.tensor(\n    [\n        [\n            [0.03192167, 0.00638559, 0.01301679],\n            [-0.83140486, 0.39370209, -0.26395324],\n            [-0.66518241, -0.84461308, 0.20759389],\n            [0.45554739, 0.54289633, 0.81170881],\n            [0.66091919, -0.16799635, -0.91037834],\n        ]\n    ]\n)\n# In periodic table, C = 6 and H = 1\nspecies = torch.tensor([[6, 1, 1, 1, 1]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And here is the result:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "energies_ensemble = model((species, coordinates)).energies\nenergies_ensemble_jit = loaded_compiled_model((species, coordinates)).energies\nprint(\n    \"Ensemble energy, eager mode vs loaded jit:\",\n    energies_ensemble.item(),\n    energies_ensemble_jit.item(),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Customize the model and script\n\nYou could also customize the model you want to export. For example, let's do\nthe following customization to the model:\n\n- uses double as dtype instead of float\n- don't care about periodic boundary condition\n- in addition to energies, allow returning optionally forces, and hessians\n\nyou could do the following:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class CustomModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = torchani.models.ANI1x().double()\n\n    def forward(\n        self,\n        species: Tensor,\n        coordinates: Tensor,\n        return_forces: bool = False,\n        return_hessians: bool = False,\n    ) -> tp.Tuple[Tensor, tp.Optional[Tensor], tp.Optional[Tensor]]:\n        if return_forces or return_hessians:\n            coordinates.requires_grad_(True)\n        energies = self.model((species, coordinates)).energies\n        _forces: tp.Optional[Tensor] = None\n        _hessians: tp.Optional[Tensor] = None\n        if return_forces or return_hessians:\n            _forces = forces(\n                energies, coordinates, retain_graph=True, create_graph=return_hessians\n            )\n            if return_hessians:\n                assert _forces is not None\n                _hessians = hessians(_forces, coordinates)\n        return energies, _forces, _hessians\n\n\ncustom_model = CustomModule()\ncompiled_custom_model = torch.jit.script(custom_model)\ntorch.jit.save(compiled_custom_model, \"compiled_custom_model.pt\")\nloaded_compiled_custom_model = torch.jit.load(\"compiled_custom_model.pt\")\nenergies_eager, forces_eager, hessians_eager = custom_model(\n    species, coordinates, True, True\n)\nenergies_jit, forces_jit, hessians_jit = loaded_compiled_custom_model(\n    species, coordinates, True, True\n)\n\nprint(\"Energy, eager mode vs loaded jit:\", energies_eager.item(), energies_jit.item())\nprint()\nprint(\n    \"Force, eager mode vs loaded jit:\\n\",\n    forces_eager.squeeze(0),\n    \"\\n\",\n    forces_jit.squeeze(0),\n)\nprint()\ntorch.set_printoptions(sci_mode=False, linewidth=1000)\nprint(\n    \"Hessian, eager mode vs loaded jit:\\n\",\n    hessians_eager.squeeze(0),\n    \"\\n\",\n    hessians_jit.squeeze(0),\n)\n# Lets delete the files we created for cleanup\nPath(\"compiled_custom_model.pt\").unlink()\nPath(\"compiled_model.pt\").unlink()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}