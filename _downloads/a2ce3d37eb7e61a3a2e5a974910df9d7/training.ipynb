{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Training an ANI network using a custom script\n\nThis example shows how to use TorchANI to train a neural network potential.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import math\nfrom pathlib import Path\n\nimport torch\nimport torch.utils.tensorboard\nfrom tqdm import tqdm\n\nimport torchani\nfrom torchani.arch import ANI, simple_ani\nfrom torchani.datasets import ANIDataset, ANIBatchedDataset, BatchedDataset\nfrom torchani.units import hartree2kcalpermol\nfrom torchani.grad import forces_for_training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Device and dataset to run the training\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nds = ANIDataset(\"../dataset/ani-1x/sample.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We prebatch the dataset to train with memory efficiency, keeping a good\nperformance.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batched_dataset_path = Path(\"./batched_dataset\").resolve()\nif not batched_dataset_path.exists():\n    torchani.datasets.create_batched_dataset(\n        ds,\n        dest_path=batched_dataset_path,\n        batch_size=2560,\n        splits={\"training\": 0.8, \"validation\": 0.2},\n    )\n\ntrain_ds: BatchedDataset = ANIBatchedDataset(batched_dataset_path, split=\"training\")\nvalid_ds: BatchedDataset = ANIBatchedDataset(batched_dataset_path, split=\"validation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use the pytorch DataLoader with multiprocessing to load the batches while we train\n\nFor more info about the DataLoader and multiprocessing read\nhttps://pytorch.org/docs/stable/data.html\n\nCACHE saves all data in memory. It is very memory intensive but faster.\nAlso, pin_memory is automatically performed by ANIBatchedDataset in the CACHE\ncase, so it should be set to False for the DataLoader.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "CACHE: bool = True\nif CACHE:\n    train_ds = train_ds.cache()\n    valid_ds = valid_ds.cache()\n\ntraining = train_ds.as_dataloader(num_workers=0)\nvalidation = valid_ds.as_dataloader(num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the transforms module to modify the batches, the API for transforms is very\nsimilar to [torchvision's API](https://pytorch.org/vision/stable/transforms.html)\nwith the difference that the transforms are applied to both target and inputs in all\ncases.\n\nTransform can be passed to the \"transform\" argument of ANIBatchedDataset to\nto be performed on-the-fly on CPU (slow if no CACHE)\n\nTransform can also be applied directly when training on GPU\n\nTransform can also be applied to a dataset when batching it, by using the\ninplace_transform argument of create_batched_dataset (Be careful, this may be\nerror prone)\n\nIn this case we wont apply any transform\n\nLets generate a model from scratch. For simplicity we use PyTorch's default random\ninitialization for the weights.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = simple_ani((\"H\", \"C\", \"N\", \"O\"), lot=\"wb97x-631gd\", repulsion=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up of optimizer and lr-scheduler\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(\n    params=model.neural_networks.parameters(),\n    lr=0.5e-3,\n    weight_decay=1e-6,\n)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    factor=0.5,\n    patience=100,\n    threshold=0,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first read the checkpoint files to restart training. We use ``latest_traininig.pt``\nto store current training state.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "latest_training_state_checkpoint_path = Path(\"./latest_training_state.pt\").resolve()\nbest_model_state_checkpoint_path = Path(\"./best_model_state.pt\").resolve()\nif latest_training_state_checkpoint_path.exists():\n    checkpoint = torch.load(latest_training_state_checkpoint_path)\n    model.load_state_dict(checkpoint[\"model\"])\n    scheduler.load_state_dict(checkpoint[\"scheduler\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n\nmodel.to(dtype=torch.float32, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "During training, we need to validate on validation set and if validation error\nis better than the best, then save the new best model to a checkpoint\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def validate(model: ANI, validation: torch.utils.data.DataLoader) -> float:\n    squared_error = 0.0\n    count = 0\n    model.train(False)\n    with torch.no_grad():\n        for properties in validation:\n            properties = {\n                k: v.to(device, non_blocking=True) for k, v in properties.items()\n            }\n            species = properties[\"species\"]\n            coordinates = properties[\"coordinates\"].float()\n            target_energies = properties[\"energies\"].float()\n            output = model((species, coordinates))\n            predicted_energies = output.energies\n            squared_error += (predicted_energies - target_energies).pow(2).sum().item()\n            count += predicted_energies.shape[0]\n    model.train(True)\n    rmse = math.sqrt(squared_error / count)\n    return hartree2kcalpermol(rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will also use TensorBoard to visualize our training process\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tensorboard = torch.utils.tensorboard.SummaryWriter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Criteria for stopping training\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "max_epochs = 5\nmin_learning_rate = 1.0e-10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Epoch 0 is right before training starts\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if scheduler.last_epoch == 0:\n    rmse = validate(model, validation)\n    print(f\"Before training starts: Validation RMSE (kcal/mol) {rmse}\")\n    scheduler.step(rmse)\n    torch.save(\n        {\n            \"model\": model.state_dict(),\n            \"optimizer\": optimizer.state_dict(),\n            \"scheduler\": scheduler.state_dict(),\n        },\n        latest_training_state_checkpoint_path,\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we come to the training loop.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mse = torch.nn.MSELoss(reduction=\"none\")\nforce_training = False\nforce_coefficient = 0.1\nfor epoch in range(scheduler.last_epoch, max_epochs + 1):\n    # Stop training if the lr is below a given threshold\n    if optimizer.param_groups[0][\"lr\"] < min_learning_rate:\n        break\n\n    # Loop over batches\n    for batch in tqdm(\n        training,\n        total=len(training),\n        desc=f\"Epoch {epoch}\",\n    ):\n        batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n        species = batch[\"species\"]\n        coordinates = batch[\"coordinates\"].float()\n        target_energies = batch[\"energies\"].float()\n        num_atoms = (species >= 0).sum(dim=1, dtype=target_energies.dtype)\n        output = model((species, coordinates))\n        predicted_energies = output.energies\n        if force_training:\n            target_forces = batch[\"forces\"].float()\n            predicted_forces = forces_for_training(predicted_energies, coordinates)\n            energy_loss = (\n                mse(predicted_energies, target_energies) / num_atoms.sqrt()\n            ).mean()\n            force_loss = (\n                mse(predicted_forces, target_forces).sum(dim=(1, 2)) / num_atoms\n            ).mean()\n            loss = energy_loss + force_coefficient * force_loss\n        else:\n            loss = (mse(predicted_energies, target_energies) / num_atoms.sqrt()).mean()\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    # Validate\n    rmse = validate(model, validation)\n    print(f\"After epoch {epoch}: Validation RMSE (kcal/mol) {rmse}\")\n\n    # Checkpoint the model if the RMSE; improved\n    if scheduler.is_better(rmse, scheduler.best):\n        torch.save(model.state_dict(), best_model_state_checkpoint_path)\n\n    # Step the epoch-scheduler\n    scheduler.step(rmse)\n\n    # Checkpoint the training state\n    torch.save(\n        {\n            \"model\": model.state_dict(),\n            \"optimizer\": optimizer.state_dict(),\n            \"scheduler\": scheduler.state_dict(),\n        },\n        latest_training_state_checkpoint_path,\n    )\n\n    # Log scalars\n    tensorboard.add_scalar(\"validation_rmse_kcalpermol\", rmse, epoch)\n    tensorboard.add_scalar(\"best_validation_rmse_kcalpermol\", scheduler.best, epoch)\n    tensorboard.add_scalar(\"learning_rate\", optimizer.param_groups[0][\"lr\"], epoch)\n    tensorboard.add_scalar(\"epoch_loss_square_ha\", loss, epoch)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}